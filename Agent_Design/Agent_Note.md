# lecture1:Agent

+ agent : 能够通过传感器（雷达、摄像头、激光、超声波等）感知其环境，并借助执行器作用于该环境的任何事物

+ 感知：任何给定时刻agent的感知输入

+ 感知序列：agent收到的所有输入数据的完整历史

+ Agent函数：抽象的数学描述，将感知序列映射为行动：    $a=\pi(h)$   (数学描述)

+ Agent程序：agent函数的具体实现，与Agent函数不同 

+ **理性Agent**：什么是理性的？

  >+ 定义成功的性能度量
  >+ Agent对环境的先验知识
  >+ Agent可以完成的行动
  >+ Agent截止到此时的感知序列
  >+ 定义理性Agent：对每一个可能的**感知序列**，根据已知的**感知序列** 提供的证据和Agent具有的**先验知识**，理性Agent应该选择能使其**性能度量最大化**的行动
  >+ 使**期望的性能**最大化，不同于全知Agent；选择动作只依赖于到此时的感知序列；能收集信息，从感知 的信息中学习；有自主性，通过学习弥补 不完整/不正确 的先验知识

+ **环境**

  >+ PEAS描述(任务环境的规范描述)：
  >  + 性能Performance
  >  + 环境Environment
  >  + 执行器Actuators
  >  + 传感器Sensors
  >
  >分类：
  >
  >+ **完全可观察的与部分可观察的**、完全不可观察的：Agent在每个时间点能否获取环境的完整状态
  >+ **单Agent与多Agent**：多Agent中通讯常作为理性行为，一些竞争环境中随机行为时理性的
  >+ **确定与随机**: 下一个状态是否完全取决于当前状态和Agent执行的动作
  >+ **片段式的与序贯式的**：
  >  + 下一个片段不依赖于以前片段中采取的行动(装配线上检验次品零件)
  >  + 当前决策会影响到所有未来的决策
  >+ **静态的与动态的**：动态环境在Agent计算的时候会发生变化；半动态：环境本身不随时间变化而变化，但是 Agent的性能评价随时间变化 
  >+ **连续的与离散的**： 

+ **Agent结构**

  >Agent=体系结构+程序
  >
  >+ 程序实现的是把感知信息映射到行动的函数
  >+ 体系结构是某个具备物理传感器和执行器的计算 装置，程序在该装置上运行
  >
  >**Agent程序**
  >
  >>+ 框架：输入为从传感器得到的当前感知信息，返 回的是执行器的行为抉择
  >>
  >>  **Agent程序以当前感知作为输入，Agent函数以感知历史作为输入**
  >>
  >>**四种基本的Agent程序**
  >>
  >>+ **简单反射Agent**: 基于当前的感知选择行动，不关注感知历史
  >>+ **基于模型的反射Agent**：用感知历史来维持内部状态,使用内部模型来跟踪环境
  >>+ **基于目标的Agent**：模型再加上目标，根据目标选择行动
  >>+ **基于效用的Agent**：效用函数驱动， 

+ **设计方法**

  >+ **显式编程**：显式地编码告诉Agent在各种可能经历的场景下该如何采取行动 ， 直接、无学习、基于规则、在简单问题上有效 
  >+ **优化**：给定策略空间和性能度量，在空间中搜索使得性 能度量最优的策略，即最优策略  ； 通过仿真实验评估策略的性能
  >+ **强化学习**：假设环境的模型未知，Agent与环境交互，从强化信号中学习 :dog:
  >
  
+ *课前练习*：

  >+ 一个Agent只能感知状态的部分信息，那么它不可能是理性的。  --false
  >+ 存在这样的任务环境，处于该环境中的纯反射Agent不可能有理性行为。 --true
  >+ 存在任务环境使得每个Agent都是理性的。 --true
  >+ Agent程序的输入与Agent函数的输入是相同的。 --false
  >+ 每个Agent函数都可以用程序/机器组合实现。 --false
  >+ 假设Agent从一组可能行动中随机选择行动。存在确定的任务环境使得此Agent是理性的。 --true
  >+ 一个给定的Agent在两个不同的任务环境中可能都是理性的。 --true
  >+ 在不可观察环境中每个Agent都是理性的。 --false
  >+ 一个理性的打牌Agent是不可能输的。 --false
  >+ 给定存储量为n比特的体系结构，可以有多少种可能的不同Agent程序？--$2^n$

  

# lecture2:概率模型

+ 信念度：

  >+ 大于为>,小于为<,等于为~
  >+ 可比性，传递性，用实值函数P来表示**信念度**（其实就是概率）

+ 条件概率(后验概率)

+ 截断式高斯分布：<img src="pic/image-20200227170216522.png"/>

+ 多模态的连续概率分布

  >+ 高斯混合模型：不同高斯分布的加权平均
  >
  >  ![image-20200227170753515](pic\image-20200227170753515.png)

+ sigmod模型![image-20200227173137285](pic\image-20200227173137285.png)

+ 贝叶斯网络
  + 给定父节点，节点X条件独立于它的非后代节点，即X仅条件依赖于它的父节点
  
  + 链式法则
  
    <img src="pic\image-20200227174122384.png" alt="image-20200227174122384" style="zoom: 50%;" />
  
  + 网络结构和有向分离：
  
    ![image-20200227212356098](pic\image-20200227212356098.png)
  
    有向分离：

    ![image-20200227213341973](pic\image-20200227213341973.png)
  
  + 马尔可夫覆盖（Markov Blanket）:
  
    + 是有向分离一个结点与其他结点的**最少个数**的结点构成的集合
    + 这个集合由该结点的父结点、子结点以及子结点的父结点构成
  
  + 混合贝叶斯网络
  
+ 有n个二值变量，则有$2^n-1$ 个独立的参数 

+ 时序模型（表示一组变量如何随时间演进）
  
  + 马尔科夫链：在任何给定的时间内，给定当前和过去状态的过程的未来状态的条件分布仅取决于当前状态，而完全不取决于过去状态（无记忆属性）。具有马尔可夫性质的随机过程称为马尔可夫过程。：  $P(X_{n+1}=s_{n+1}|X_n=s_n,X_{n-1}=s_{n-1},...)=P(X_{n+1}=s_{n+1}|X_n=s_n)$ 
  + 隐马尔可夫模型(HMM): see https://blog.csdn.net/lukabruce/article/details/82380511. 



# lecture3:概率推理

### 贝叶斯网络中的推理

+ 推理：由一组**证据变量**的值来确定一个或多个**查询变量**的分布

+ 枚举推理：常用到全概率公式和链式法则

  ![image-20200305175916574](pic\image-20200305175916574.png)

  + 表格：表示离散型的联合概率分布或条件概率分布

  + 因子相乘：结合两个因子以产生一个更大的因子：
    $$
    \alpha=P(X,Y),\beta=P(Y,Z),\alpha*\beta=P(X,Y,Z)
    $$
    表现为对两个表格中Y相同的值的部分相乘

  + 因子边际化：对因子中某一变量的所有值求和以产生一个小一点的因子
    $$
    \sum_{y\in Y} P(X,Y,Z)=P(X,Z)
    $$

  + 设置证据：对因子中的证据变量赋值，产生一个更小的因子
    $$
    P(X,Y,Z),令Y=1，得到P(X,Z)
    $$



### 分类推理

+ 用于分类任务，从给定的一组观察或特征中推理所属类别

+ 朴素贝叶斯模型：一种简单的概率模型，常用于分类任务，（朴素）假设：给定所属 类别，证据变量之间条件 独立，若假设不成立，可以在观察到 的特征间添加必要的有向边

  + 盘式(plate)记法：

  + 在朴素贝叶斯模型中，需要指出先验概率𝑃(𝐶)和条件于类别 的分布$𝑃(𝑂_i)|C)$ 

  + 推导：
    $$
    已知P(c)和P(O_i|c),求解P(c|o_{1:n})\\
    链式规则：P(c,o_{1:n})=P(c)\prod_{i=1}^nP(o_i|c)\\
    条件概率：P(c|o_{1:n})=\frac{P(c,o_{1:n})}{P(o_{1:n})}=\frac{P(c,o_{1:n})}{\sum_c{P(c,o_{1:n})}}\\
    分母可求是个常数，故通常写为: P(c|o_{1:n})\propto P(c,o_{1:n})
    $$



### 时序模型中的推理

+ 假设时序模型为隐马尔可夫模型
+ 常见推理任务：
  + 滤波：$P(s_t|o_{0:t})$，即已知观测点求当下时刻的状态
  + 预测：$P(s_t'|o_{0:t})$：t'>t
  + 平滑：$P(s_t'|o_{0:t})$：t'<t
  + 寻找最可能的状态序列 :  $argmax_{s_{0:t}} P(s_{0:t}|o_{0:t})$ ,

+ 滤波求解算法：

  ![](pic\image-20200305190713303.png)

  第二行是由于$P(A|c,B)=\frac{P(A,B|c)}{P(B|c)}=\frac{P(B|A,c)P(A|c)}{P(B|c)}$, 即$P(A,B)=P(A|B)P(B)=P(B|A)P(A)$，贝叶斯公式。

  **递归贝叶斯估计（前向算法）**：

  ![](pic\image-20200305191623132.png)



+ 预测：

  ![image-20200305192001355](pic\image-20200305192001355.png)



+ 平滑：

  <img src="pic\image-20200305192056832.png" alt="image-20200305192056832" style="zoom: 80%;" />

  前向-后向算法：

  ![image-20200305192222204](pic\image-20200305192222204.png)

  一直计算直到k+1=t，此时第二项没有了



+ 寻找最可能序列：

  ![image-20200305192403940](pic\image-20200305192403940.png)





### 精确推理

+ 变量消去法：

  ![image-20200305193312033](pic\image-20200305193312033.png)

  先列出相关的表，然后通过设置证据消去d和c相关的表得到新表并加入，然后对某一个隐变量相关的表进行因子相乘后关于此隐变量因子边际化，不停进行直到所有隐变量消去，得到的表归一化后即可求出

  ![image-20200305193340581](pic\image-20200305193340581.png)

  



+ 信念传播法：

  将变量消去法中的求和看作一个**消息传递过程**

  <img src="pic\image-20200305194503728.png" alt="image-20200305194503728" style="zoom: 50%;" />

  结点的边际分布正比于它所接收的消息的乘积:
  $$
  P(x_i)\propto \prod_{k\in n(i)}{m_{ki}(x_i)}
  $$
  一个结点仅在接收到来自其他所有结点的消息后才能向另一个结点发送消息

  计算所有变量的边际分布：

  <img src="pic\image-20200305195023430.png" alt="image-20200305195023430" style="zoom:67%;" />





### 精确推理的复杂度

+ 复杂度类：
  +  P：可以在多项式时间内求解的问题
  +  NP：解可以在多项式时间内得到验证的问题
  +  NP-难：难度不小于最难的NP问题的问题 
  + NP-完全：同属于NP和NP-难的问题 
+ 证明一个问题Q是NP-难的：归约（通常的做法）把一个已知的NP-完全问题转化为Q的一个实例 
+ 贝叶斯网络中的推理是NP-难的



### 近似推理

+ 直接采样法：采n个样本，取其中满足条件的个数n1,n2，则概率为$P=\frac{n_1}{n_2}$

  ![image-20200311191404837](pic\image-20200311191404837.png)

+ 贝叶斯网络中的拓扑排序

  + ![image-20200311190920895](pic\image-20200311190920895.png)
  + ![image-20200311191616917](pic\image-20200311191616917.png)
  + 缺点：产生了许多与观察不一致的样本

+ 似然加权法：

  + 特点：产生与观察一致的加权样本

    <img src="pic\image-20200311192454523.png" alt="image-20200311192454523" style="zoom:67%;" />

    <img src="pic\image-20200311192544106.png" alt="image-20200311192544106" style="zoom:67%;" />

    + NIL表示当前为查询变量，则直接采样
    
      ![image-20200313202520026](pic\image-20200313202520026.png)
    
      In order to compute the probability of an event, *X=true*,  that is dependent on another event,    *Y=true*, we sum the weights of all samples where *X=true* **and**     *Y=true* and divide it by the sum of the weights of all samples where *Y=true*.    For example, if we want to compute p(a | j), we need to sum the weights of all samples where    we have both a and j (meaning *Alarm=True* and *JohnCalls=True*).  We find that    only sample 3 meets this criteria with a weight of 0.63.  We now sum the weights of all samples    that have j.  Only samples 2 and 3 meet this criteria with weights 0.10 and 0.63, respectively.    Putting this all together, we have p(a | j) = 0.63 / (0.10 + 0.63) = 0.63 / 0.73 = 0.863.
    
      see https://my.eng.utah.edu/~mccully/cs5300lw/

+ 马尔科夫链蒙特卡洛(Markov Chain Monte Carlo，MCMC)：包括吉布斯采样算法(Gibbs)、模拟退火算法等

  + 构造一条马尔可夫链，使其收敛至稳态分布，其恰为待估计参数的后验分布
  + 通过这条马尔可夫链来随机产生符合后验分布的样本，并基于这些样本来进行估计

+ 吉布斯采样法：从任意样本（将证据变量固定为观察值）出发，通过对非证据变量逐个进行采样改变其取值，生成下一个样本

  + <img src="pic\image-20200311193645682.png" alt="image-20200311193645682" style="zoom:67%;" />
  
  + <img src="pic\image-20200311193934924.png" alt="image-20200311193934924" style="zoom:67%;" />
  
  + 例子：（来自人工智能一种现代的方法）求$P(Rain|Sprinkler=true,WetGrass=true)$
  
    ![image-20200313205404860](pic\image-20200313205404860.png)
  
    它的算法描述是：![image-20200313205610106](pic\image-20200313205610106.png)
  
    另一个描述：
  
    ![image-20200313205515795](pic\image-20200313205515795.png)





# lecture4:参数学习

### 极大似然参数学习：离散与连续模型

+ 最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

  ![image-20200313211109347](pic\image-20200313211109347.png)

+ 找出使得似然函数最大的参数
  + ![image-20200311194454835](D:\Git\20spring-CourseNote\Agent_Design\pic\image-20200311194454835.png)
  + (概率论复习)
  + 步骤：
    + 为数据的似然性写下一个表达式，即参数的函数
    + 写下对数似然关于每个参数的偏导数
    +   推导出使导数为0的参数值

+ 多项分布：
  + ![image-20200311195013771](pic\image-20200311195013771.png)
  + ![image-20200311195122770](pic\image-20200311195122770.png)

### 贝叶斯参数学习

+ 给定数据，计算每个假说的概率，并基于这些概率做决策 
+  用所有假说做预测，而不是使用单个“最好”的假说 
+ 把学习归约于概率推理
+ 参数学习中的贝叶斯方法：基于假说先验，估计$\theta$的后验分布

+ 伽玛函数$\Gamma(n)=(n-1)!$.
+ 贝塔（Beta）分布可以作为二项分布参数的先验分布 
+  若选贝塔分布作为先验，后验也是贝塔分布
+ 若先验为Beta(𝛼, 𝛽)，观察为𝑜i 如果𝑜i=1，则后验为𝐵eta(𝛼 + 1, 𝛽) ;如果𝑜i=0，则后验为𝐵eta(𝛼, 𝛽 + 1)
+ 狄利克雷（Dirichlet）分布：贝塔分布的广义形式

### 非参数化模型的密度估算

+ 参数化模型：用固定数目参数组成的集合（独立于训练样本的数目）概括数据的学习模型
+ 非参数化模型：一类不能用有限参数集合刻画的函数 
  + 参数的个数可以随数据的数量发生变化，假说空间是非参数化的
  + 密度估算：k-最近邻模型、核函数