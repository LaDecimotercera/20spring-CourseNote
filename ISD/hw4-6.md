# 课后练习4-6章

***人工智能学院 181220076 周韧哲***





### 4.1

+ 马尔科夫假设是指状态$S_{t+1}$和奖赏$R_t$仅依赖于时刻$t$的状态和行动，与之前的状态和行动无关。

+ 马尔科夫决策过程由状态空间、行动空间、奖赏空间和动力函数构成。

+ 稳态MDP是指动力函数不随时间发生变化的MDP，从而状态转移函数和奖赏函数也不随时间变化。

+ 稳态MDP的决策网络表示：

  <img src="pic\QQ图片20200408174152.png" style="zoom: 33%;" />

### 4.2

（以下用$\pi_l,\pi_r$分别表示$\pi_{left},\pi_{right}$）

+ 当$\gamma=0$时，策略是短视的，仅考虑当前奖赏，有$U^{\pi_{l}}=1,U^{\pi_{r}}=0$，故$\pi_{l}$为最优策略

+ 当$\gamma=0.5$时

  + 在$\pi_{l}$下，$G_t=\sum_{k=0}^\infin0.5^k\times 1$，$U^{\pi_{l}}=E_{\pi_{l}}[G_t]=\lim_{k\rightarrow\infin}2-\frac{1}{2^k}=2$。
  + 在$\pi_r$下，$G_t=\sum_{k=0}^\infin0.5^k\times 2$，$U^{\pi_{r}}=E_{\pi_{l}}[G_t]=\lim_{k\rightarrow\infin}4-\frac{1}{2^{k-1}}=4$。

  所以$\pi_r$为最优策略

+ 当$\gamma=0.9$时

  + 在$\pi_{l}$下，$G_t=\sum_{k=0}^\infin0.9^k\times 1$，$U^{\pi_{l}}=E_{\pi_{l}}[G_t]=\lim_{k\rightarrow\infin}10\times(1-0.9^{k+1})=10$。
  + 在$\pi_r$下，$G_t=\sum_{k=0}^\infin0.9^k\times 2$，$U^{\pi_{r}}=E_{\pi_{l}}[G_t]=\lim_{k\rightarrow\infin}20\times(1-0.9^{k+1})=20$。

  所以$\pi_r$为最优策略



### 4.3

+ 由
  $$
  U^\pi(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma U^\pi(s')]
$$
  容易写出每个状态的状态值函数：
  $$
  \begin{align}
  U^\pi(h)&=\pi(s|h)[\alpha(r_{s}+\gamma U^\pi(h))+(1-\alpha)(r_{s}+\gamma U^\pi(l))]+\\
  &\quad \pi(w|h)[1\times(r_{w}+\gamma U^\pi(h))]\\
  &=\pi(s|h)[\alpha\gamma U^\pi(h)+(1-\alpha)\gamma U^\pi(l)+r_s]+\\
  &\quad\pi(w|h)[r_{w}+\gamma U^\pi(h)]\\ \\
  U^\pi(l)&=\pi(s|l)[\beta(r_{s}+\gamma U^\pi(l))+(1-\beta)(-3+\gamma U^\pi(h))]+\\
  &\quad\pi(w|l)[1\times(r_{w}+\gamma U^\pi(l))]+\\
  &\quad\pi(re|l)[0+\gamma U^\pi(h)]\\
  &=\pi(s|l)[\beta\gamma U^\pi(l)+(1-\beta)\gamma U^\pi(h)+\beta r_s-3(1-\beta)]+\\
  &\quad\pi(w|l)[r_{w}+\gamma U^\pi(l)]+\\
  &\quad\pi(re|l)[\gamma U^\pi(h)]
  \end{align}
  $$
  则其最优状态值函数的Bellman方程为：
  $$
  \begin{align}
  U^*(h)&=\max\{\alpha\gamma U^*(h)+(1-\alpha)\gamma U^*(l)+r_s,\\
  &\qquad\qquad r_w+\gamma U^*(h) \}\\\\
  U^*(l)&=\max\{ \beta\gamma U^*(l)+(1-\beta)\gamma U^*(h)+\beta r_s-3(1-\beta),\\
  &\qquad\qquad r_{w}+\gamma U^*(l),\\
  &\qquad\qquad\gamma U^*(h)\}
  \end{align}
  $$
  





### 4.5

+ $$
  \begin{align}
  \|U^*(s)-U_{k}(s)\|_\infin&=\max_s |U^*(s)-U_k(s)|\\
  &= \gamma\max_s|\max_a\sum_{s'}T(s'|s,a)U^*(s')-\max_a\sum_{s'}T(s'|s,a)U_{k-1}(s') |\\
  &\leq\gamma\max_s \max_a |\sum_{s'}T(s'|s,a)|U^*(s')-\sum_{s'}T(s'|s,a)U_{k-1}(s')|\\
  &=\gamma\max_s|\sum_{s'}T(s'|s,a^*)(U^*(s')-U_{k-1}(s'))|  \\
  &\leq \gamma\sum_{s'}T(s'|s,a^*)\max_{s'}|U^*(s')-U_{k-1}(s')|\\
  &=\gamma\|U^*(s)-U_{k-1}(s)\|_\infin 
  \end{align}
  $$

  同时，有：
  $$
  \begin{align}
  \|U^*(s)-U_{k-1}(s)\|_\infin-\|U^*(s)-U_{k}(s)\|_\infin&=\max_{s'}|U^*(s')-U_{k-1}(s')|-\max_{s'}|U^*(s')-U_{k}(s')|\\
  &\leq\max_{s'}|U_{k}(s')-U_{k-1}(s')|\\
  &=\|U_k(s')-U_{k-1}(s')\|_\infin
  \end{align}
  $$
  综合可得
  $$
  \begin{align}
  \|U^*(s)-U_{k-1}(s)\|_\infin-\|U^*(s)-U_{k}(s)\|_\infin&=(\frac{1}{\gamma}-1)
  \|U^*(s)-U_{k}(s)\|_\infin\\
  &\leq \|U_k(s)-U_{k-1}(s)\|_\infin \\
  &< \delta \\
  &=\epsilon(\frac{1}{\gamma}-1)
  \end{align}
  $$
  同时除以$\frac{1}{\gamma}-1$，即得证：
  $$
  \|U^*(s)-U_k(s)\|_\infin<\epsilon
  $$

