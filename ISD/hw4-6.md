# 课后练习4-6章

***人工智能学院 181220076 周韧哲***





### 4.1

+ 马尔科夫假设是指状态$S_{t+1}$和奖赏$R_t$仅依赖于时刻$t$的状态和行动，与之前的状态和行动无关。

+ 马尔科夫决策过程由状态空间、行动空间、奖赏空间和动力函数构成。

+ 稳态MDP是指动力函数不随时间发生变化的MDP，从而状态转移函数和奖赏函数也不随时间变化。

+ 稳态MDP的决策网络表示：

  <img src="pic\QQ图片20200408174152.png" style="zoom: 33%;" />



### 4.3

+ 由
  $$
  U^\pi(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma U^\pi(s')]
  $$
  容易写出每个状态的状态值函数：
  $$
  \begin{align}
  U^\pi(h)&=\pi(s|h)[\alpha(r_{s}+\gamma U^\pi(h))+(1-\alpha)(r_{s}+\gamma U^\pi(l))]+\\
  &\quad \pi(w|h)[1\times(r_{w}+\gamma U^\pi(h))]\\
  &=\pi(s|h)[\alpha\gamma U^\pi(h)+(1-\alpha)\gamma U^\pi(l)+r_s]+\\
  &\quad\pi(w|h)[r_{w}+\gamma U^\pi(h)]\\ \\
  U^\pi(l)&=\pi(s|l)[\beta(r_{s}+\gamma U^\pi(l))+(1-\beta)(-3+\gamma U^\pi(h))]+\\
  &\quad\pi(w|l)[1\times(r_{w}+\gamma U^\pi(l))]+\\
  &\quad\pi(re|l)[0+\gamma U^\pi(h)]\\
  &=\pi(s|l)[\beta\gamma U^\pi(l)+(1-\beta)\gamma U^\pi(h)+\beta r_s-3(1-\beta)]+\\
  &\quad\pi(w|l)[r_{w}+\gamma U^\pi(l)]+\\
  &\quad\pi(re|l)[\gamma U^\pi(h)]
  \end{align}
  $$
  则其最优状态值函数的Bellman方程为：
  $$
  \begin{align}
  U^*(h)&=\max\{\alpha\gamma U^*(h)+(1-\alpha)\gamma U^*(l)+r_s,\\
  &\qquad\qquad r_w+\gamma U^*(h) \}\\\\
  U^*(l)&=\max\{ \beta\gamma U^*(l)+(1-\beta)\gamma U^*(h)+\beta r_s-3(1-\beta),\\
  &\qquad\qquad r_{w}+\gamma U^*(l),\\
  &\qquad\qquad\gamma U^*(h)\}
  \end{align}
  $$
  

